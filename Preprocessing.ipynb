{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTEGRAD Project </h2>\n",
    "<h3>PREPROCESSING</h3>\n",
    "\n",
    "<hr>\n",
    "<span style=\"font-variant: small-caps;\">Xavier Jiménez, Jean Quentin, Sacha Revol</span><br>\n",
    "<hr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph import data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Graph is loaded from `edgelist.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    }
   ],
   "source": [
    "# Create a graph\n",
    "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess `authors.txt` and save it as a dictionary `authors_preprocessed.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20330it [00:00, 203034.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138499it [00:01, 130319.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['james h niblock', 'jianxun peng', 'karen r mcmenemy', 'george w irwin']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Preprocessing authors')\n",
    "authors = dict()\n",
    "punctuation = \"!#$%&'()*+-./:<=>?@[\\]^_`{|}~\"\n",
    "with open('data/authors.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        node, author = line.split('|--|')\n",
    "        author = author.lower()\n",
    "        author = \"\".join([char for char in author if char not in punctuation])\n",
    "        author = author.split(',')        \n",
    "        author[-1] = author[-1].strip()\n",
    "        authors[int(node)] = author\n",
    "    a_file = open(\"data/authors_preprocessed.pkl\", \"wb\")\n",
    "    pickle.dump(authors, a_file)\n",
    "    a_file.close()\n",
    "print('Preprocessing Done')\n",
    "authors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Authors preprocessed\n"
     ]
    }
   ],
   "source": [
    "# Read the abstract of each paper\n",
    "try:\n",
    "    print('Loading Authors preprocessed')\n",
    "    a_file = open(\"data/authors_preprocessed.pkl\", \"rb\")\n",
    "    authors = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "except:\n",
    "    raise SyntaxError(\"File 'authors_preprocessed.pkl' was not found in 'data/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionnary from `authors_preprocessed.pkl` with unique names (e.j. \"X. Jimenez\" and \"Xavier Jimenez\" are considered as the same author) and saved it as `unique_authors.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from namematcher import NameMatcher\n",
    "from tqdm import tqdm \n",
    "\n",
    "def name_similarity(name1, name2):\n",
    "    \"\"\"Computes similarity between two names. If similarity is above 0.9 and both names have the\n",
    "    same letter for the first name, it will return True if one of the authors first name has only\n",
    "    one letter. False otherwise. If score is 1, returns True.\n",
    "\n",
    "    Args:\n",
    "        name1 (str): author name\n",
    "        name2 (str): author name\n",
    "        debug (bool, optional): If True, will print authors name and similarity score. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if names are the same. False otherwise.\n",
    "    \"\"\"\n",
    "    if name1 == name2:\n",
    "        return True\n",
    "    elif name1.split(\" \")[-1] != name2.split(\" \")[-1]:\n",
    "        return False\n",
    "    \n",
    "    name_matcher = NameMatcher()\n",
    "    score = name_matcher.match_names(name1, name2)\n",
    "\n",
    "    if score >= 0.9:\n",
    "        if name1.split(\" \")[0][0] == name2.split(\" \")[0][0]:\n",
    "            if len(name1.split(\" \")[0]) == 1 and len(name2.split(\" \")[0]) > 1:\n",
    "                return True\n",
    "            elif len(name2.split(\" \")[0]) == 1 and len(name1.split(\" \")[0]) > 1:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138499/138499 [00:00<00:00, 1257017.84it/s]\n"
     ]
    }
   ],
   "source": [
    "authors_list = list()\n",
    "for i in tqdm(range(len(authors))):\n",
    "    for author in authors[i]:\n",
    "        authors_list.append(author)\n",
    "authors_list = list(set(authors_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146122/146122 [5:00:00<00:00,  8.12it/s]\n"
     ]
    }
   ],
   "source": [
    "authors_dict = dict()\n",
    "for i in tqdm(range(0, len(authors_list))):\n",
    "    similar_authors = list()\n",
    "    for j in range(0, len(authors_list)):\n",
    "        if name_similarity(authors_list[i], authors_list[j]):\n",
    "            similar_authors.append(authors_list[j])\n",
    "        elif i == j:\n",
    "            similar_authors.append(authors_list[i])\n",
    "    authors_dict[authors_list[i]] = np.sort(similar_authors)\n",
    "    \n",
    "a_file = open(\"data/similar_authors_dict.pkl\", \"wb\")\n",
    "pickle.dump(authors_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading similar authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138499/138499 [00:01<00:00, 99087.57it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Loading similar authors')\n",
    "    a_file = open(\"data/similar_authors_dict.pkl\", \"rb\")\n",
    "    authors_dict = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "except:\n",
    "    raise SyntaxError(\"File 'similar_authors_dict.pkl' was not found in 'data/'\")\n",
    "\n",
    "unique_authors = dict()\n",
    "for i in tqdm(range(len(authors))):\n",
    "    authors_list = list()\n",
    "    for author in authors[i]:\n",
    "        if len(authors_dict[author]) < 3:\n",
    "            authors_list.append(authors_dict[author][-1])\n",
    "        else:\n",
    "            authors_list.append(author)\n",
    "    unique_authors[i] = authors_list\n",
    "\n",
    "a_file = open(\"data/unique_authors_dict.pkl\", \"wb\")\n",
    "pickle.dump(unique_authors, a_file)\n",
    "a_file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unique authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138499/138499 [00:00<00:00, 604902.17it/s]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Loading unique authors')\n",
    "    a_file = open(\"data/unique_authors_dict.pkl\", \"rb\")\n",
    "    unique_authors = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "except:\n",
    "    raise SyntaxError(\"File 'unique_authors_dict.pkl' was not found in 'data/'\")\n",
    "\n",
    "f = open(\"data/unique_authors.txt\", 'w')\n",
    "for i in tqdm(range(len(unique_authors))):\n",
    "    f.write('{:d}|--|'.format(i) + \",\".join(unique_authors[i]) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138499/138499 [00:00<00:00, 1272804.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456810\n",
      "146122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "authors_list = list()\n",
    "for i in tqdm(range(len(authors))):\n",
    "    for author in authors[i]:\n",
    "        authors_list.append(author)\n",
    "print(len(authors_list))\n",
    "print(len(list(set(authors_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess `abstracts.txt` and save it as a dictionary `abstract_preprocessed.pkl`. Common NLP operations are made: lowercase, remove punctuation, tokenize, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessing abstracts')\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "abstracts = dict()\n",
    "with open('data/abstracts.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstract = abstract.lower()\n",
    "        abstract = \"\".join([char for char in abstract if char not in string.punctuation])\n",
    "        abstract = word_tokenize(abstract)\n",
    "        abstract = [word for word in abstract if word not in stop_words]\n",
    "        # abstract = [porter.stem(word) for word in abstract]\n",
    "        abstracts[int(node)] = abstract\n",
    "a_file = open(\"data/abstract_preprocessed.pkl\", \"wb\")\n",
    "pickle.dump(abstracts, a_file)\n",
    "a_file.close()\n",
    "print('Preprocessing Done')\n",
    "abstracts[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Graph is loaded from `edgelist.txt` and random edges are removed from it (p=5%). These removed edges will be used to create a validation set that mirrors the test set `test.txt`. New graph is saved as `edgelist_val.txt` and will be loaded as H instead of G. If p is changed, the value should be changed as well on other .ipynb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "G = StellarGraph.from_networkx(nx_G, node_type_default=\"paper\", edge_type_default=\"cites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an edge splitter on the original graph:\n",
    "edge_splitter_test = data.EdgeSplitter(G)\n",
    "\n",
    "# Randomly sample a fraction p=0.1 of all positive links, and same number of negative links, from graph, and obtain the\n",
    "# reduced graph graph_test with the sampled links removed:\n",
    "graph_train_test, examples_test, labels_test = edge_splitter_test.train_test_split(p=0.25, \n",
    "                                                                             method=\"global\", \n",
    "                                                                             keep_connected=True, \n",
    "                                                                             seed=42)\n",
    "\n",
    "nx.write_edgelist(graph_train_test.to_networkx(),\n",
    "                  'data/edgelist_test.txt',\n",
    "                  delimiter=',',\n",
    "                  data=False)\n",
    "\n",
    "# Save test pairs\n",
    "pd.DataFrame(examples_test).to_csv('data/train_test_node_pairs.csv', index=False, header=False)\n",
    "# Save test targets\n",
    "pd.DataFrame(labels_test).to_csv('data/train_test_labels.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same process to compute a training subset from within the test graph\n",
    "edge_splitter_train = data.EdgeSplitter(graph_train_test, G)\n",
    "graph_train_val, examples, labels = edge_splitter_train.train_test_split(p=0.2, \n",
    "                                                                     method=\"global\", \n",
    "                                                                     keep_connected=True, \n",
    "                                                                     seed=42)\n",
    "\n",
    "nx.write_edgelist(graph_train_val.to_networkx(),\n",
    "                  'data/edgelist_train.txt',\n",
    "                  delimiter=',',\n",
    "                  data=False)\n",
    "\n",
    "# Save training pairs\n",
    "pd.DataFrame(examples).to_csv('data/train_val_node_pairs.csv', index=False, header=False)\n",
    "# Save training targets\n",
    "pd.DataFrame(labels).to_csv('data/train_val_labels.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2843cabd0433704bf9af0eb427e5fac31b8e3c43971b08ae8afc4b2111159d22"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
