{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTEGRAD Project </h2>\n",
    "<h3>PREPROCESSING</h3>\n",
    "\n",
    "<hr>\n",
    "<span style=\"font-variant: small-caps;\">Xavier Jim√©nez, Jean Quentin, Sacha Revol</span><br>\n",
    "<hr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# !pip install pip install karateclub\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from os import path\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "import random\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Graph is loaded from `edgelist.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess `authors.txt` and save it as a dictionary `authors_preprocessed.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessing authors')\n",
    "authors = dict()\n",
    "with open('data/authors.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        node, author = line.split('|--|')\n",
    "        # author = author.lower()\n",
    "        author = author.split(',')\n",
    "        author[-1] = author[-1].strip()\n",
    "        authors[int(node)] = author\n",
    "    a_file = open(\"data/authors_preprocessed.pkl\", \"wb\")\n",
    "    pickle.dump(authors, a_file)\n",
    "    a_file.close()\n",
    "print('Preprocessing Done')\n",
    "authors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess `abstracts.txt` and save it as a dictionary `abstract_preprocessed.pkl`. Common NLP operations are made: lowercase, remove punctuation, tokenize, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessing abstracts')\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "abstracts = dict()\n",
    "with open('data/abstracts.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstract = abstract.lower()\n",
    "        abstract = \"\".join([char for char in abstract if char not in string.punctuation])\n",
    "        abstract = word_tokenize(abstract)\n",
    "        abstract = [word for word in abstract if word not in stop_words]\n",
    "        # abstract = [porter.stem(word) for word in abstract]\n",
    "        abstracts[int(node)] = abstract\n",
    "a_file = open(\"data/abstract_preprocessed.pkl\", \"wb\")\n",
    "pickle.dump(abstracts, a_file)\n",
    "a_file.close()\n",
    "print('Preprocessing Done')\n",
    "abstracts[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Graph is loaded from `edgelist.txt` and random edges are removed from it (p=5%). These removed edges will be used to create a validation set that mirrors the test set `test.txt`. New graph is saved as `edgelist_val.txt` and will be loaded as H instead of G. If p is changed, the value should be changed as well on other .ipynb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.05\n",
    "    \n",
    "H = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "\n",
    "with open('data/edgelist.txt') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "random.seed(42)\n",
    "indices_to_delete = random.sample(range(len(lines)), int(p * len(lines)))\n",
    "\n",
    "# sort to delete biggest index first \n",
    "indices_to_delete.sort(reverse=True)\n",
    "\n",
    "for i in tqdm(indices_to_delete):\n",
    "    line = lines[i]\n",
    "    t = line.split(',')\n",
    "    H.remove_edge(int(t[0]), int(t[1]))\n",
    "\n",
    "nx.readwrite.edgelist.write_edgelist(H, 'data/edgelist_val.txt', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
