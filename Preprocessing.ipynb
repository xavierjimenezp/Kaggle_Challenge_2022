{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>ALTEGRAD Project </h2>\n",
    "<h3>PREPROCESSING</h3>\n",
    "\n",
    "<hr>\n",
    "<span style=\"font-variant: small-caps;\">Xavier Jiménez, Jean Quentin, Sacha Revol</span><br>\n",
    "<hr>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, make_scorer\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# !pip install pip install karateclub\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from os import path\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Graph is loaded from `edgelist.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 138499\n",
      "Number of edges: 1091955\n"
     ]
    }
   ],
   "source": [
    "# Create a graph\n",
    "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "nodes = list(G.nodes())\n",
    "n = G.number_of_nodes()\n",
    "m = G.number_of_edges()\n",
    "print('Number of nodes:', n)\n",
    "print('Number of edges:', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess `authors.txt` and save it as a dictionary `authors_preprocessed.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20330it [00:00, 203034.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing authors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138499it [00:01, 130319.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['james h niblock', 'jianxun peng', 'karen r mcmenemy', 'george w irwin']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Preprocessing authors')\n",
    "authors = dict()\n",
    "punctuation = \"!#$%&'()*+-./:<=>?@[\\]^_`{|}~\"\n",
    "with open('data/authors.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        node, author = line.split('|--|')\n",
    "        author = author.lower()\n",
    "        author = \"\".join([char for char in author if char not in punctuation])\n",
    "        author = author.split(',')        \n",
    "        author[-1] = author[-1].strip()\n",
    "        authors[int(node)] = author\n",
    "    a_file = open(\"data/authors_preprocessed.pkl\", \"wb\")\n",
    "    pickle.dump(authors, a_file)\n",
    "    a_file.close()\n",
    "print('Preprocessing Done')\n",
    "authors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Authors preprocessed\n"
     ]
    }
   ],
   "source": [
    "# Read the abstract of each paper\n",
    "try:\n",
    "    print('Loading Authors preprocessed')\n",
    "    a_file = open(\"data/authors_preprocessed.pkl\", \"rb\")\n",
    "    authors = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "except:\n",
    "    raise SyntaxError(\"File 'authors_preprocessed.pkl' was not found in 'data/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionnary from `authors_preprocessed.pkl` with unique names (e.j. \"X. Jimenez\" and \"Xavier Jimenez\" are considered as the same author) and saved it as `unique_authors.pkl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from namematcher import NameMatcher\n",
    "from tqdm import tqdm \n",
    "\n",
    "def name_similarity(name1, name2):\n",
    "    \"\"\"Computes similarity between two names. If similarity is above 0.9 and both names have the\n",
    "    same letter for the first name, it will return True if one of the authors first name has only\n",
    "    one letter. False otherwise. If score is 1, returns True.\n",
    "\n",
    "    Args:\n",
    "        name1 (str): author name\n",
    "        name2 (str): author name\n",
    "        debug (bool, optional): If True, will print authors name and similarity score. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if names are the same. False otherwise.\n",
    "    \"\"\"\n",
    "    if name1 == name2:\n",
    "        return True\n",
    "    elif name1.split(\" \")[-1] != name2.split(\" \")[-1]:\n",
    "        return False\n",
    "    \n",
    "    name_matcher = NameMatcher()\n",
    "    score = name_matcher.match_names(name1, name2)\n",
    "\n",
    "    if score >= 0.9:\n",
    "        if name1.split(\" \")[0][0] == name2.split(\" \")[0][0]:\n",
    "            if len(name1.split(\" \")[0]) == 1 and len(name2.split(\" \")[0]) > 1:\n",
    "                return True\n",
    "            elif len(name2.split(\" \")[0]) == 1 and len(name1.split(\" \")[0]) > 1:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138499/138499 [00:00<00:00, 1140952.64it/s]\n"
     ]
    }
   ],
   "source": [
    "authors_list = list()\n",
    "for i in tqdm(range(len(authors))):\n",
    "    for author in authors[i]:\n",
    "        authors_list.append(author)\n",
    "authors_list = list(set(authors_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:26<00:00,  7.54it/s]\n"
     ]
    }
   ],
   "source": [
    "authors_dict = dict()\n",
    "for i in tqdm(range(0, 200)):#len(authors_list))):\n",
    "    similar_authors = list()\n",
    "    for j in range(0, len(authors_list)):\n",
    "        if name_similarity(authors_list[i], authors_list[j]):\n",
    "            similar_authors.append(authors_list[j])\n",
    "        elif i == j:\n",
    "            similar_authors.append(authors_list[i])\n",
    "    authors_dict[authors_list[i]] = np.sort(similar_authors)\n",
    "    \n",
    "a_file = open(\"data/similar_authors_dict.pkl\", \"wb\")\n",
    "pickle.dump(authors_dict, a_file)\n",
    "a_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Loading similar authors')\n",
    "    a_file = open(\"data/similar_authors_dict.pkl\", \"rb\")\n",
    "    authors_dict = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "except:\n",
    "    raise SyntaxError(\"File 'similar_authors_dict.pkl' was not found in 'data/'\")\n",
    "\n",
    "unique_authors = dict()\n",
    "for i in tqdm(range(len(authors))):\n",
    "    authors_list = list()\n",
    "    for author in authors[i]:\n",
    "        authors_list.append(authors_dict[author][0])\n",
    "unique_authors[i] = authors_list\n",
    "\n",
    "a_file = open(\"data/unique_authors_dict.pkl\", \"wb\")\n",
    "pickle.dump(unique_authors, a_file)\n",
    "a_file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess `abstracts.txt` and save it as a dictionary `abstract_preprocessed.pkl`. Common NLP operations are made: lowercase, remove punctuation, tokenize, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Preprocessing abstracts')\n",
    "stop_words = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "abstracts = dict()\n",
    "with open('data/abstracts.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f):\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstract = abstract.lower()\n",
    "        abstract = \"\".join([char for char in abstract if char not in string.punctuation])\n",
    "        abstract = word_tokenize(abstract)\n",
    "        abstract = [word for word in abstract if word not in stop_words]\n",
    "        # abstract = [porter.stem(word) for word in abstract]\n",
    "        abstracts[int(node)] = abstract\n",
    "a_file = open(\"data/abstract_preprocessed.pkl\", \"wb\")\n",
    "pickle.dump(abstracts, a_file)\n",
    "a_file.close()\n",
    "print('Preprocessing Done')\n",
    "abstracts[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citation Graph is loaded from `edgelist.txt` and random edges are removed from it (p=5%). These removed edges will be used to create a validation set that mirrors the test set `test.txt`. New graph is saved as `edgelist_val.txt` and will be loaded as H instead of G. If p is changed, the value should be changed as well on other .ipynb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.05\n",
    "    \n",
    "H = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
    "\n",
    "with open('data/edgelist.txt') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "random.seed(42)\n",
    "indices_to_delete = random.sample(range(len(lines)), int(p * len(lines)))\n",
    "\n",
    "# sort to delete biggest index first \n",
    "indices_to_delete.sort(reverse=True)\n",
    "\n",
    "for i in tqdm(indices_to_delete):\n",
    "    line = lines[i]\n",
    "    t = line.split(',')\n",
    "    H.remove_edge(int(t[0]), int(t[1]))\n",
    "\n",
    "nx.readwrite.edgelist.write_edgelist(H, 'data/edgelist_val.txt', delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2843cabd0433704bf9af0eb427e5fac31b8e3c43971b08ae8afc4b2111159d22"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
