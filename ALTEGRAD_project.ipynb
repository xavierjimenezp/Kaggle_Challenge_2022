{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhGtlA7RYYpV"
      },
      "source": [
        "<center><h2>ALTEGRAD Project</h2>\n",
        "\n",
        "<hr>\n",
        "<span style=\"font-variant: small-caps;\">Xavier Jiménez</span><br>\n",
        "<hr>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1FMzZfeYMgQ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCeqWnCtUBiW",
        "outputId": "1caa3353-4986-4b40-e960-35483c5c1a34"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# !pip install pip install karateclub\n",
        "from karateclub import DeepWalk\n",
        "from os import path\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already downloaded\n"
          ]
        }
      ],
      "source": [
        "if not path.isdir('data'):\n",
        "    !mkdir data\n",
        "    !wget -O altegrad.zip https://www.dropbox.com/sh/fhfjjtk0sr7pmse/AAD4ZEtHv9OI5HfVO22tdMX0a?dl=1\n",
        "    !unzip altegrad.zip\n",
        "else:\n",
        "    print('Data already downloaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufV7kEnYJFo"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWDhmJpnVmCl",
        "outputId": "d75081de-3cb0-4049-86a9-2df0cf559397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes: 138499\n",
            "Number of edges: 1091955\n"
          ]
        }
      ],
      "source": [
        "# Create a graph\n",
        "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "nodes = list(G.nodes())\n",
        "n = G.number_of_nodes()\n",
        "m = G.number_of_edges()\n",
        "print('Number of nodes:', n)\n",
        "print('Number of edges:', m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5j-YMepW46s",
        "outputId": "ec1883d8-fac2-423c-eb09-3e747c18f393"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "27it [00:00, 264.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing abstracts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "138499it [05:49, 395.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Done\n"
          ]
        }
      ],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Read the abstract of each paper\n",
        "try:\n",
        "    a_file = open(\"data/abstract_preprocessed.pkl\", \"rb\")\n",
        "    abstracts = pickle.load(a_file)\n",
        "    a_file.close()\n",
        "    print('Abstract already preprocessed')\n",
        "except:\n",
        "    print('Preprocessing abstracts')\n",
        "    abstracts = dict()\n",
        "    with open('data/abstracts.txt', 'r') as f:\n",
        "        for line in tqdm(f):\n",
        "            node, abstract = line.split('|--|')\n",
        "            abstract = abstract.lower()\n",
        "            abstract = \"\".join([char for char in abstract if char not in string.punctuation])\n",
        "            abstract = word_tokenize(abstract)\n",
        "            abstract = [word for word in abstract if word not in stop_words]\n",
        "            abstract = [porter.stem(word) for word in abstract]\n",
        "            abstracts[int(node)] = set(abstract)\n",
        "    a_file = open(\"data/abstract_preprocessed.pkl\", \"wb\")\n",
        "    pickle.dump(abstracts, a_file)\n",
        "    a_file.close()\n",
        "    print('Preprocessing Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5LbuWc0eF6U"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def return_embeddings(G, model, parameters):\n",
        "    \"\"\"Creates embeddings for a given model\n",
        "\n",
        "    Args:\n",
        "        G: nx Graph.\n",
        "        model: Graph embedding model from karateclub library.\n",
        "        parameters (dict): Dictionary containing model parameters.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: embeddings.\n",
        "    \"\"\"\n",
        "    \n",
        "    emb = model(**parameters)\n",
        "    emb.fit(G)\n",
        "    \n",
        "    return emb.get_embedding()\n",
        "\n",
        "def validation_score(model, X_train, y_train, cv, scoring = 'neg_log_loss', n_jobs = None):\n",
        "    \"\"\"Computes scores using cross validation for a given model.\n",
        "\n",
        "    Args:\n",
        "        model: classifier\n",
        "        X_train (array like): training set.\n",
        "        y_train (array like): training lavels.\n",
        "        cv (int): number of splits.\n",
        "        scoring (str, optional): Metric. Defaults to 'neg_log_loss'.\n",
        "        n_jobs (int, optional): Number of cores. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        list: list containing scores from cross validation\n",
        "    \"\"\"\n",
        "    \n",
        "    scores = cross_val_score(model, X_train, y_train, cv = cv, scoring = 'neg_log_loss', n_jobs = n_jobs)\n",
        "    print('Score: {:.2f} ± {:.2f}'.format(-scores.mean(), scores.std()/np.sqrt(cv)))\n",
        "    \n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK_z5HvGeBaR"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nZfxf1PIbJrN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2957it [00:00, 29489.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features computed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1091955it [00:32, 33215.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of training matrix: (2183910, 7)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Create the training matrix. Each row corresponds to a pair of nodes and\n",
        "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
        "# Use the following 3 features for each pair of nodes:\n",
        "# (1) sum of number of unique terms of the two nodes' abstracts\n",
        "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
        "# (3) number of common terms between the abstracts of the two nodes\n",
        "\n",
        "# (1) sum of degrees of two nodes\n",
        "# (2) absolute value of difference of degrees of two nodes\n",
        "\n",
        "# computes structural features for each node\n",
        "G.remove_edges_from(nx.selfloop_edges(G))\n",
        "core_number = nx.core_number(G)\n",
        "# onion_number = nx.onion_layers(G)\n",
        "# avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
        "# degree_centrality = nx.degree_centrality(G)\n",
        "# clustering = nx.clustering(G)\n",
        "print(\"Features computed\")\n",
        "\n",
        "X_train = np.zeros((2*m, 7))\n",
        "y_train = np.zeros(2*m)\n",
        "for i,edge in tqdm(enumerate(G.edges())):\n",
        "    # an edge\n",
        "    X_train[2*i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
        "    X_train[2*i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
        "    X_train[2*i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
        "    X_train[2*i,3] = G.degree(edge[0]) + G.degree(edge[1])\n",
        "    X_train[2*i,4] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
        "    X_train[2*i,5] = core_number[edge[0]] + core_number[edge[1]]\n",
        "    X_train[2*i,6] = abs(core_number[edge[0]] - core_number[edge[1]])\n",
        "    y_train[2*i] = 1\n",
        "\n",
        "    # a randomly generated pair of nodes\n",
        "    n1 = randint(0, n-1)\n",
        "    n2 = randint(0, n-1)\n",
        "    X_train[2*i+1,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
        "    X_train[2*i+1,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
        "    X_train[2*i+1,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
        "    X_train[2*i+1,3] = G.degree(n1) + G.degree(n2)\n",
        "    X_train[2*i+1,4] = abs(G.degree(n1) - G.degree(n2))\n",
        "    X_train[2*i,5] = core_number[n1] + core_number[n2]\n",
        "    X_train[2*i,6] = abs(core_number[n1] - core_number[n2])\n",
        "    y_train[2*i+1] = 0\n",
        "\n",
        "print('Size of training matrix:', X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W93zJLSmT8sM",
        "outputId": "93063418-2968-4160-9c12-689ac192e631"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xavier/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/home/xavier/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/home/xavier/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 0.00 ± 0.00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xavier/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression()\n",
        "scores = validation_score(clf, X_train, y_train, cv = 4, n_jobs = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG2eMnlKdbOj"
      },
      "source": [
        "## DeepWalk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_0EoR98sbyLh"
      },
      "outputs": [],
      "source": [
        "parameters = {'walk_number': 20, 'walk_length': 80, 'dimensions': 128, 'workers': 3}\n",
        "dw_embeddings = return_embeddings(G, DeepWalk, parameters)\n",
        "print(type(dw_embeddings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from karateclub import Node2Vec\n",
        "parameters = {'walk_number': 10, 'walk_length': 80, 'dimensions': 128, 'workers': 3, 'window_size': 5}\n",
        "n2v_embeddings = return_embeddings(G, Node2Vec, parameters)\n",
        "np.save('data/embedding_n2v_wn10_wl80_d128_ws5', n2v_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = np.zeros((2*m, 7))\n",
        "y_train = np.zeros(2*m)\n",
        "for i,edge in tqdm(enumerate(G.edges())):\n",
        "    # an edge\n",
        "    X_train[2*i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
        "    X_train[2*i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
        "    X_train[2*i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
        "    X_train[2*i,3] = G.degree(edge[0]) + G.degree(edge[1])\n",
        "    X_train[2*i,4] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
        "    X_train[2*i,5] = dw_embeddings[edge[0],:]\n",
        "    X_train[2*i,6] = dw_embeddings[edge[1],:]\n",
        "    y_train[2*i] = 1\n",
        "\n",
        "    # a randomly generated pair of nodes\n",
        "    n1 = randint(0, n-1)\n",
        "    n2 = randint(0, n-1)\n",
        "    X_train[2*i+1,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
        "    X_train[2*i+1,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
        "    X_train[2*i+1,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
        "    X_train[2*i+1,3] = G.degree(n1) + G.degree(n2)\n",
        "    X_train[2*i+1,4] = abs(G.degree(n1) - G.degree(n2))\n",
        "    X_train[2*i,5] = dw_embeddings[edge[n1],:]\n",
        "    X_train[2*i,6] = dw_embeddings[edge[n2],:]\n",
        "    y_train[2*i+1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\n",
        "scores = validation_score(clf, X_train, y_train, cv = 4, n_jobs = 4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ALTEGRAD_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
