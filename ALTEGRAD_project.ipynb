{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhGtlA7RYYpV"
      },
      "source": [
        "<center><h2>ALTEGRAD Project</h2>\n",
        "\n",
        "<hr>\n",
        "<span style=\"font-variant: small-caps;\">Xavier Jiménez</span><br>\n",
        "<hr>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1FMzZfeYMgQ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCeqWnCtUBiW",
        "outputId": "1caa3353-4986-4b40-e960-35483c5c1a34"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import log_loss, make_scorer\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# !pip install pip install karateclub\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "from os import path\n",
        "import pickle\n",
        "from scipy import spatial\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already downloaded\n"
          ]
        }
      ],
      "source": [
        "if not path.isdir('data'):\n",
        "    !mkdir data\n",
        "    !wget -O altegrad.zip https://www.dropbox.com/sh/fhfjjtk0sr7pmse/AAD4ZEtHv9OI5HfVO22tdMX0a?dl=1\n",
        "    !unzip altegrad.zip\n",
        "else:\n",
        "    print('Data already downloaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufV7kEnYJFo"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWDhmJpnVmCl",
        "outputId": "d75081de-3cb0-4049-86a9-2df0cf559397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes: 138499\n",
            "Number of edges: 1091955\n"
          ]
        }
      ],
      "source": [
        "# Create a graph\n",
        "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "nodes = list(G.nodes())\n",
        "n = G.number_of_nodes()\n",
        "m = G.number_of_edges()\n",
        "print('Number of nodes:', n)\n",
        "print('Number of edges:', m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authors already preprocessed\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['James H. Niblock', 'Jian-Xun Peng', 'Karen R. McMenemy', 'George W. Irwin']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the abstract of each paper\n",
        "try:\n",
        "    a_file = open(\"data/authors_preprocessed.pkl\", \"rb\")\n",
        "    authors = pickle.load(a_file)\n",
        "    a_file.close()\n",
        "    print('Authors already preprocessed')\n",
        "except:\n",
        "    print('Preprocessing authors')\n",
        "    authors = dict()\n",
        "    with open('data/authors.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f):\n",
        "            node, author = line.split('|--|')\n",
        "            # author = author.lower()\n",
        "            author = author.split(',')\n",
        "            author[-1] = author[-1].strip()\n",
        "            authors[int(node)] = author\n",
        "        a_file = open(\"data/authors_preprocessed.pkl\", \"wb\")\n",
        "        pickle.dump(authors, a_file)\n",
        "        a_file.close()\n",
        "    print('Preprocessing Done')\n",
        "\n",
        "authors[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5j-YMepW46s",
        "outputId": "ec1883d8-fac2-423c-eb09-3e747c18f393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Abstract already preprocessed\n",
            "Preprocessing Done\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['development',\n",
              " 'automated',\n",
              " 'system',\n",
              " 'quality',\n",
              " 'assessment',\n",
              " 'aerodrome',\n",
              " 'ground',\n",
              " 'lighting',\n",
              " 'agl',\n",
              " 'accordance']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the abstract of each paper\n",
        "try:\n",
        "    a_file = open(\"data/abstract_preprocessed.pkl\", \"rb\")\n",
        "    abstracts = pickle.load(a_file)\n",
        "    a_file.close()\n",
        "    print('Abstract already preprocessed')\n",
        "except:\n",
        "    print('Preprocessing abstracts')\n",
        "    stop_words = stopwords.words('english')\n",
        "    porter = PorterStemmer()\n",
        "    abstracts = dict()\n",
        "    with open('data/abstracts.txt', 'r', encoding=\"utf-8\") as f:\n",
        "        for line in tqdm(f):\n",
        "            node, abstract = line.split('|--|')\n",
        "            abstract = abstract.lower()\n",
        "            abstract = \"\".join([char for char in abstract if char not in string.punctuation])\n",
        "            abstract = word_tokenize(abstract)\n",
        "            abstract = [word for word in abstract if word not in stop_words]\n",
        "            # abstract = [porter.stem(word) for word in abstract]\n",
        "            abstracts[int(node)] = abstract\n",
        "    a_file = open(\"data/abstract_preprocessed.pkl\", \"wb\")\n",
        "    pickle.dump(abstracts, a_file)\n",
        "    a_file.close()\n",
        "\n",
        "print('Preprocessing Done')\n",
        "abstracts[0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5LbuWc0eF6U"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_random_edges_from_G(p):\n",
        "    \"\"\"Removes p lines from edgelist randomly and saves\n",
        "    remaining lines as edgelist_missing.txt\n",
        "\n",
        "    Args:\n",
        "        p (float): line percentage to be removed\n",
        "    \"\"\"\n",
        "\n",
        "    H = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "    with open('data/edgelist.txt') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    random.seed(42)\n",
        "    indices_to_delete = random.sample(range(len(lines)), int(p * len(lines)))\n",
        "\n",
        "    # sort to delete biggest index first \n",
        "    indices_to_delete.sort(reverse=True)\n",
        "\n",
        "    for i in tqdm(indices_to_delete):\n",
        "        line = lines[i]\n",
        "        t = line.split(',')\n",
        "        H.remove_edge(int(t[0]), int(t[1]))\n",
        "        \n",
        "    return H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation_score(model, X_train, y_train, cv, scoring = 'log_loss', n_jobs = None, verbose = 0):\n",
        "    \"\"\"Computes scores using cross validation for a given model.\n",
        "\n",
        "    Args:\n",
        "        model: classifier\n",
        "        X_train (array like): training set.\n",
        "        y_train (array like): training lavels.\n",
        "        cv (int): number of splits.\n",
        "        scoring (str, optional): Metric. Defaults to 'neg_log_loss'.\n",
        "        n_jobs (int, optional): Number of cores. Defaults to None.\n",
        "        verbose (int, optional): Verbose level for cross_val_score\n",
        "\n",
        "    Returns:\n",
        "        list: list containing scores from cross validation\n",
        "    \"\"\"\n",
        "    print('Cross validation')\n",
        "    if scoring == 'log_loss':\n",
        "        scoring = make_scorer(log_loss, greater_is_better = False, needs_proba = True)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv = cv, scoring = scoring, n_jobs = n_jobs, verbose = verbose)\n",
        "    print('Score: {:.2f} ± {:.2f}'.format(-scores.mean(), scores.std()/np.sqrt(cv)))\n",
        "    \n",
        "    return scores\n",
        "\n",
        "def create_submission(model, X_test, X_train, y_train):\n",
        "    # # Read test data. Each sample is a pair of nodes\n",
        "    print('Creating submission')\n",
        "    node_pairs = list()\n",
        "    with open('data/test.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            t = line.split(',')\n",
        "            node_pairs.append((int(t[0]), int(t[1])))\n",
        "\n",
        "    \n",
        "    # Use model to predict if two nodes are linked by an edge\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict_proba(X_test)\n",
        "    y_pred = y_pred[:,1]\n",
        "\n",
        "    # Write predictions to a file\n",
        "    predictions = zip(range(len(y_pred)), y_pred)\n",
        "    os.remove(\"data/submission.csv\")\n",
        "    with open(\"data/submission.csv\",\"w\") as pred:\n",
        "        csv_out = csv.writer(pred)\n",
        "        csv_out.writerow(['id','predicted'])\n",
        "        for row in predictions:\n",
        "            csv_out.writerow(row)\n",
        "    print('Submision created')\n",
        "\n",
        "\n",
        "def graph_properties(G):\n",
        "    print('Computing graph properties')\n",
        "    avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
        "    pagerank = nx.pagerank_scipy(G)\n",
        "\n",
        "    return [avg_neighbor_degree, pagerank]\n",
        "            \n",
        "def create_dataset(G, H, G_params): \n",
        "        \n",
        "    node_pairs = list()\n",
        "    with open('data/test.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            t = line.split(',')\n",
        "            node_pairs.append((int(t[0]), int(t[1])))\n",
        "            \n",
        "    X_train = np.zeros((2*len(G.edges()), 8 + 2*len(G_params)))\n",
        "    y_train = np.zeros(2*len(G.edges()))\n",
        "    X_test = np.zeros((len(node_pairs), 8 + 2*len(G_params)))\n",
        "    \n",
        "    n2v_parameters = {'walk_number': 10, 'walk_length': 15, 'dimensions': 64, 'window_size': 5, 'workers': 7}\n",
        "    d2v_parameters = {'vector_size':128, 'window':5, 'min_count':2, 'epochs':100, 'workers':7}\n",
        "    \n",
        "    d2v = Doc2Vec.load(\"data/abstracts_embedding_doc2vec_vs{:d}_w{:d}_mc{:d}_e{:d}\".format(d2v_parameters['vector_size'], d2v_parameters['window'],\n",
        "                                                                               d2v_parameters['min_count'], d2v_parameters['epochs']))\n",
        "    n2v = np.load('data/embedding_n2v_wn{:d}_wl{:d}_d{:d}_ws{:d}.npy'.format(n2v_parameters['walk_number'], n2v_parameters['walk_length'],\n",
        "                                                               n2v_parameters['dimensions'], n2v_parameters['window_size']))\n",
        "    \n",
        "    # gae = pd.read_csv('data/Z_GAE_32_tanh_Ep2000_0.1063.csv')\n",
        "    gae = None\n",
        "    \n",
        "    distance = spatial.distance.cosine #euclidean_distance #\n",
        "    \n",
        "    for i,edge in tqdm(enumerate(G.edges())):\n",
        "        X_train, y_train = fill_matrix(X_train, y_train, i, edge, 2, H, G_params, d2v, n2v, gae, distance, training = True)\n",
        "    for i,edge in tqdm(enumerate(node_pairs)):\n",
        "        X_test = fill_matrix(X_test, None, i, edge, 1, G, G_params, d2v, n2v, gae, distance, training = False)\n",
        "               \n",
        "    print('Size of the matrix:', X_train.shape)\n",
        "    \n",
        "    return X_train, X_test, y_train\n",
        "\n",
        "def euclidean_distance(x, y):   \n",
        "    return np.sqrt(np.sum((x - y) ** 2))\n",
        "\n",
        "def fill_matrix(X, y_train, i, edge, p, H, G_params, d2v, n2v, gae, distance, training):\n",
        "    # an edge\n",
        "    X[p*i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
        "    X[p*i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
        "    X[p*i,2] = len(set(abstracts[edge[0]]).intersection(set(abstracts[edge[1]])))\n",
        "    X[p*i,3] = H.degree(edge[0]) + H.degree(edge[1])\n",
        "    X[p*i,4] = abs(H.degree(edge[0]) - H.degree(edge[1]))\n",
        "    X[p*i,5] = len(set(authors[edge[0]]).intersection(set(authors[edge[1]])))\n",
        "    X[p*i,6] = distance(d2v[edge[0]], d2v[edge[1]])\n",
        "    X[p*i,7] = distance(n2v[edge[0]], n2v[edge[1]])\n",
        "    # X[p*i,8] = distance(gae.iloc[edge[0]].to_numpy(), gae.iloc[edge[1]].to_numpy())\n",
        "    for idx, j in enumerate(range(8, 8 + len(G_params) + 1, 2)):\n",
        "        param = G_params[idx]\n",
        "        X[p*i,j] = param[edge[0]] + param[edge[1]]\n",
        "        X[p*i,j+1] = abs(param[edge[0]] - param[edge[1]])\n",
        "    \n",
        "    if training:\n",
        "        y_train[2*i] = 1\n",
        "\n",
        "        # a randomly generated pair of nodes\n",
        "        # random.seed(2*i) \n",
        "        n1 = randint(0, n-1)\n",
        "        # random.seed(2*i+1) \n",
        "        n2 = randint(0, n-1)\n",
        "            \n",
        "        X[2*i+1,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
        "        X[2*i+1,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
        "        X[2*i+1,2] = len(set(abstracts[n1]).intersection(set(abstracts[n2])))\n",
        "        X[2*i+1,3] = H.degree(n1) + H.degree(n2)\n",
        "        X[2*i+1,4] = abs(H.degree(n1) - H.degree(n2))\n",
        "        X[2*i+1,5] = len(set(authors[n1]).intersection(set(abstracts[n2])))\n",
        "        X[2*i+1,6] = distance(d2v[n1], d2v[n2])\n",
        "        X[2*i+1,7] = distance(n2v[n1], n2v[n2])\n",
        "        # X[2*i+1,8] = distance(gae.iloc[n1].to_numpy(), gae.iloc[n2].to_numpy())\n",
        "        for idx, j in enumerate(range(8, 8 + len(G_params) + 1, 2)):\n",
        "            param = G_params[idx]\n",
        "            X[2*i+1,j] = param[n1] + param[n2]\n",
        "            X[2*i+1,j+1] = abs(param[n1] - param[n2])\n",
        "\n",
        "        y_train[2*i+1] = 0\n",
        "\n",
        "        return X, y_train\n",
        "    else:\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10919/10919 [00:00<00:00, 264744.41it/s]\n"
          ]
        }
      ],
      "source": [
        "H = remove_random_edges_from_G(p = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing graph properties\n"
          ]
        }
      ],
      "source": [
        "# Computes node parameters for graph G and stores them in a list\n",
        "G_params = graph_properties(H)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1091955it [02:44, 6628.14it/s]\n",
            "106692it [00:08, 13308.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the matrix: (2183910, 12)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train = create_dataset(G, H, G_params = G_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>157.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.734795</td>\n",
              "      <td>7.073539</td>\n",
              "      <td>50.150000</td>\n",
              "      <td>8.850000</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>207.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.894699</td>\n",
              "      <td>12.509662</td>\n",
              "      <td>100.555556</td>\n",
              "      <td>33.722222</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>193.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.000406</td>\n",
              "      <td>6.734310</td>\n",
              "      <td>38.807692</td>\n",
              "      <td>20.192308</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>191.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>18.291679</td>\n",
              "      <td>8.461211</td>\n",
              "      <td>164.063158</td>\n",
              "      <td>30.463158</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>182.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.918881</td>\n",
              "      <td>6.587539</td>\n",
              "      <td>32.286364</td>\n",
              "      <td>9.013636</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1     2     3     4    5          6          7           8   \\\n",
              "0  157.0  15.0   2.0  22.0  18.0  1.0   6.734795   7.073539   50.150000   \n",
              "1  207.0   5.0   8.0  72.0   0.0  0.0  13.894699  12.509662  100.555556   \n",
              "2  193.0  21.0   7.0  41.0  37.0  0.0   6.000406   6.734310   38.807692   \n",
              "3  191.0  63.0   3.0  43.0  33.0  0.0  18.291679   8.461211  164.063158   \n",
              "4  182.0  40.0  10.0  31.0   9.0  0.0   5.918881   6.587539   32.286364   \n",
              "\n",
              "          9         10        11  \n",
              "0   8.850000  0.000015  0.000011  \n",
              "1  33.722222  0.000033  0.000006  \n",
              "2  20.192308  0.000034  0.000030  \n",
              "3  30.463158  0.000012  0.000005  \n",
              "4   9.013636  0.000023  0.000003  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train = pd.DataFrame(data=X_train)\n",
        "df_test = pd.DataFrame(data=X_test)\n",
        "# df_train = df_train.iloc[:,:]\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test results with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross validation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=7)]: Using backend LokyBackend with 7 concurrent workers.\n",
            "/home/xavier/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/home/xavier/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/home/xavier/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 0.23 ± 0.02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=7)]: Done   4 out of   4 | elapsed:  1.4min finished\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(max_iter = 200)\n",
        "scores = validation_score(clf, df_train, y_train, cv = 4, n_jobs = 7, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating submission\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xavier/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Submision created\n"
          ]
        }
      ],
      "source": [
        "clf = LogisticRegression(max_iter = 200)\n",
        "create_submission(clf, df_test, df_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing embedding properties and other ideas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nodes_connected(G, u, v):\n",
        "    return u in G.neighbors(v)\n",
        "\n",
        "nodes_connected(G, 17, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected 0-0: 0.09, 4\n",
            "Connected 3-0: 0.59, 0\n",
            "Connected 5-0: 1.10, 0\n",
            "Connected 6-0: 0.65, 0\n",
            "Connected 7-0: 0.61, 0\n",
            "Connected 9-0: 0.69, 0\n",
            "Connected 10-0: 0.70, 1\n",
            "Connected 11-0: 0.54, 0\n",
            "Connected 12-0: 0.61, 0\n"
          ]
        }
      ],
      "source": [
        "d2v = Doc2Vec.load(\"data/abstracts_embedding_doc2vec_vs64_w5_mc2_e100\")\n",
        "\n",
        "neighbor = nx.single_source_shortest_path_length(G, 1, cutoff=3)\n",
        "# neighbor = {v: k for k, v in neighbor.items()}\n",
        "\n",
        "for i in list(neighbor.keys())[1:10]:\n",
        "    if nodes_connected(G, i, 1):\n",
        "        print('Connected {:d}-{:d}: {:.2f}, {:d}'.format(i, 0, spatial.distance.cosine(d2v[i], d2v[1]), len(authors[i].intersection(authors[0]))))\n",
        "    else:\n",
        "        print('Not Connected {:d}-{:d}: {:.2f}, {:d}'.format(i, 0, spatial.distance.cosine(d2v[i], d2v[1]), len(authors[i].intersection(authors[0]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not Connected 57843-0: 1.05\n",
            "Not Connected 37538-0: 0.93\n",
            "Not Connected 65147-0: 0.87\n",
            "Not Connected 106678-0: 0.93\n",
            "Not Connected 18223-0: 0.95\n",
            "Not Connected 89973-0: 0.91\n",
            "Not Connected 125515-0: 0.95\n",
            "Not Connected 73222-0: 1.09\n",
            "Not Connected 64208-0: 0.99\n",
            "Not Connected 4594-0: 0.94\n",
            "Not Connected 19913-0: 0.95\n",
            "Not Connected 57703-0: 0.89\n",
            "Not Connected 23905-0: 0.99\n",
            "Not Connected 122321-0: 0.92\n",
            "Not Connected 9201-0: 1.02\n",
            "Not Connected 101385-0: 0.98\n",
            "Not Connected 61178-0: 0.75\n",
            "Not Connected 88102-0: 0.87\n",
            "Not Connected 111660-0: 0.78\n",
            "Not Connected 90555-0: 1.03\n"
          ]
        }
      ],
      "source": [
        "for i in range(0,20):\n",
        "    # random.seed(i)\n",
        "    k1 = randint(0, n-1)\n",
        "    k2 = randint(0, n-1)\n",
        "    if nodes_connected(G, k1, k2):\n",
        "        print('Connected {:d}-{:d}: {:.2f}'.format(k1, 0, spatial.distance.cosine(d2v[k1], d2v[k2])))\n",
        "    else:\n",
        "        print('Not Connected {:d}-{:d}: {:.2f}'.format(k2, 0, spatial.distance.cosine(d2v[k1], d2v[k2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected 0-1: 0.09\n",
            "Connected 0-2: 0.86\n",
            "Connected 1-3: 0.59\n",
            "Connected 1-5: 1.10\n",
            "Connected 1-6: 0.65\n",
            "Connected 1-7: 0.61\n",
            "Connected 1-9: 0.69\n",
            "Connected 1-10: 0.70\n",
            "Connected 1-11: 0.54\n",
            "Connected 1-12: 0.61\n",
            "Connected 1-13: 0.58\n",
            "Connected 1-14: 0.59\n",
            "Connected 1-15: 0.67\n",
            "Connected 1-16: 0.66\n",
            "Connected 1-17: 0.84\n",
            "Connected 1-19: 1.00\n",
            "Connected 1-20: 0.66\n",
            "Connected 1-21: 0.74\n",
            "Connected 1-22: 0.60\n",
            "Connected 1-23: 0.61\n",
            "Connected 1-24: 0.72\n",
            "Connected 2-25: 0.82\n",
            "Connected 2-26: 0.91\n",
            "Connected 2-27: 0.99\n",
            "Connected 2-28: 1.00\n",
            "Connected 2-29: 0.99\n",
            "Connected 2-30: 1.05\n",
            "Connected 2-31: 1.04\n",
            "Connected 2-32: 0.87\n",
            "Connected 2-33: 0.92\n",
            "Connected 2-34: 0.93\n"
          ]
        }
      ],
      "source": [
        "for i, edge in enumerate(G.edges()):\n",
        "    print('Connected {:d}-{:d}: {:.2f}'.format(edge[0], edge[1], spatial.distance.cosine(d2v[edge[0]], d2v[edge[1]])))\n",
        "    if i==30:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20it [00:00, 4169.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for i,edge in tqdm(enumerate(G.edges())):\n",
        "    print(len(set(authors[edge[0]]).intersection(set(authors[edge[1]]))))\n",
        "    # cos_distance = spatial.distance.cosine(d2v[edge[0]], d2v[edge[1]])\n",
        "    # print(cos_distance)\n",
        "   \n",
        "    if i == 20:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ALTEGRAD_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
