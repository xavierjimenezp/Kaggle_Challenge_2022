{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhGtlA7RYYpV"
      },
      "source": [
        "<center><h2>ALTEGRAD Project</h2>\n",
        "\n",
        "<hr>\n",
        "<span style=\"font-variant: small-caps;\">Xavier Jiménez</span><br>\n",
        "<hr>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1FMzZfeYMgQ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCeqWnCtUBiW",
        "outputId": "1caa3353-4986-4b40-e960-35483c5c1a34"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# !pip install pip install karateclub\n",
        "from karateclub import DeepWalk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufV7kEnYJFo"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWDhmJpnVmCl",
        "outputId": "d75081de-3cb0-4049-86a9-2df0cf559397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes: 138499\n",
            "Number of edges: 1091955\n"
          ]
        }
      ],
      "source": [
        "# Create a graph\n",
        "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "nodes = list(G.nodes())\n",
        "n = G.number_of_nodes()\n",
        "m = G.number_of_edges()\n",
        "print('Number of nodes:', n)\n",
        "print('Number of edges:', m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5j-YMepW46s",
        "outputId": "ec1883d8-fac2-423c-eb09-3e747c18f393"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "porter = PorterStemmer()\n",
        "\n",
        "# Read the abstract of each paper\n",
        "try:\n",
        "    df_abstracts = pd.read_csv('data/abstract_preprocessed.csv')\n",
        "except:\n",
        "    abstracts = dict()\n",
        "    with open('data/abstracts.txt', 'r') as f:\n",
        "        for line in tqdm(f):\n",
        "            node, abstract = line.split('|--|')\n",
        "            abstract = abstract.lower()\n",
        "            abstract = \"\".join([char for char in abstract if char not in string.punctuation])\n",
        "            abstract = word_tokenize(abstract)\n",
        "            abstract = [word for word in abstract if word not in stop_words]\n",
        "            abstract = [porter.stem(word) for word in abstract]\n",
        "            abstracts[int(node)] = set(abstract)\n",
        "    df_abstracts = pd.DataFrame(data=abstract)\n",
        "    df_abstracts.to_csv('data/abstract_preprocessed.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5LbuWc0eF6U"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK_z5HvGeBaR"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZfxf1PIbJrN"
      },
      "outputs": [],
      "source": [
        "# Create the training matrix. Each row corresponds to a pair of nodes and\n",
        "# its class label is 1 if it corresponds to an edge and 0, otherwise.\n",
        "# Use the following 3 features for each pair of nodes:\n",
        "# (1) sum of number of unique terms of the two nodes' abstracts\n",
        "# (2) absolute value of difference of number of unique terms of the two nodes' abstracts\n",
        "# (3) number of common terms between the abstracts of the two nodes\n",
        "\n",
        "# (1) sum of degrees of two nodes\n",
        "# (2) absolute value of difference of degrees of two nodes\n",
        "\n",
        "X_train = np.zeros((2*m, 5))\n",
        "y_train = np.zeros(2*m)\n",
        "for i,edge in tqdm(enumerate(G.edges())):\n",
        "    # an edge\n",
        "    X_train[2*i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
        "    X_train[2*i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
        "    X_train[2*i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
        "    X_train[2*i,3] = G.degree(edge[0]) + G.degree(edge[1])\n",
        "    X_train[2*i,4] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
        "    y_train[2*i] = 1\n",
        "\n",
        "    # a randomly generated pair of nodes\n",
        "    n1 = randint(0, n-1)\n",
        "    n2 = randint(0, n-1)\n",
        "    X_train[2*i+1,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
        "    X_train[2*i+1,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
        "    X_train[2*i+1,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
        "    X_train[2*i+1,3] = G.degree(n1) + G.degree(n2)\n",
        "    X_train[2*i+1,4] = abs(G.degree(n1) - G.degree(n2))\n",
        "    y_train[2*i+1] = 0\n",
        "\n",
        "print('Size of training matrix:', X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W93zJLSmT8sM",
        "outputId": "93063418-2968-4160-9c12-689ac192e631"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1091955it [00:52, 20615.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of training matrix: (2183910, 5)\n",
            "Score: 0.34 ± 0.03\n"
          ]
        }
      ],
      "source": [
        "def validation_score(model, X_train, y_train, cv, scoring = 'neg_log_loss', n_jobs = None):\n",
        "\n",
        "    scores = cross_val_score(model, X_train, y_train, cv = cv, scoring = 'neg_log_loss', n_jobs = 4)\n",
        "    print('Score: {:.2f} ± {:.2f}'.format(-scores.mean(), scores.std()/np.sqrt(cv)))\n",
        "    \n",
        "    return scores\n",
        "\n",
        "clf = LogisticRegression()\n",
        "scores = validation_score(clf, X_train, y_train, cv = 4, n_jobs = 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG2eMnlKdbOj"
      },
      "source": [
        "## Deepwalk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RG1f51LdlPo"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Deep Learning on Graphs - ALTEGRAD - Dec 2020\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from random import randint\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "# Task 1\n",
        "# Simulates a random walk of length \"walk_length\" starting from node \"node\"\n",
        "def random_walk(G, node, walk_length):\n",
        "\n",
        "    walk = [node]\n",
        "    for _ in range(walk_length-1):\n",
        "        neighbors = list(G.neighbors(walk[-1]))\n",
        "        random_neighbor = neighbors[randint(0, len(neighbors)-1)]\n",
        "        walk.append(random_neighbor)\n",
        "\n",
        "    walk = [str(node) for node in walk]\n",
        "    return walk\n",
        "\n",
        "\n",
        "# Task 2\n",
        "# Runs \"num_walks\" random walks from each node\n",
        "def generate_walks(G, num_walks, walk_length):\n",
        "    walks = []\n",
        "\n",
        "    nodes = list(G.nodes())\n",
        "    for _ in range(num_walks):\n",
        "        permuted_nodes = np.random.permutation(nodes)\n",
        "        for node in permuted_nodes:\n",
        "            walks.append(random_walk(G, node, walk_length))\n",
        "\n",
        "    return walks\n",
        "\n",
        "# Simulates walks and uses the Skipgram model to learn node representations\n",
        "\n",
        "\n",
        "def deepwalk(G, num_walks, walk_length, n_dim):\n",
        "    print(\"Generating walks\")\n",
        "    walks = generate_walks(G, num_walks, walk_length)\n",
        "\n",
        "    print(\"Training word2vec\")\n",
        "    model = Word2Vec(size=n_dim, window=8, min_count=0, sg=1, workers=8)\n",
        "    model.build_vocab(walks)\n",
        "    model.train(walks, total_examples=model.corpus_count, epochs=5)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0EoR98sbyLh"
      },
      "outputs": [],
      "source": [
        "n_dim = 128\n",
        "n_walks = 20\n",
        "walk_length = 80\n",
        "\n",
        "model = DeepWalk(walk_length=walk_length, walk_number=n_walks, dimensions=n_dim, workers=2)\n",
        "model.fit(G)\n",
        "embeddings = model.get_embedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPZzMT7NlHRx"
      },
      "outputs": [],
      "source": [
        "X_train = np.zeros((2*m, 7))\n",
        "y_train = np.zeros(2*m)\n",
        "for i,edge in tqdm(enumerate(G.edges())):\n",
        "    # an edge\n",
        "    X_train[2*i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
        "    X_train[2*i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
        "    X_train[2*i,2] = len(abstracts[edge[0]].intersection(abstracts[edge[1]]))\n",
        "    X_train[2*i,3] = G.degree(edge[0]) + G.degree(edge[1])\n",
        "    X_train[2*i,4] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
        "    X_train[2*i,5] = embeddings[edge[0],:]\n",
        "    X_train[2*i,6] = embeddings[edge[1],:]\n",
        "    y_train[2*i] = 1\n",
        "\n",
        "    # a randomly generated pair of nodes\n",
        "    n1 = randint(0, n-1)\n",
        "    n2 = randint(0, n-1)\n",
        "    X_train[2*i+1,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
        "    X_train[2*i+1,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
        "    X_train[2*i+1,2] = len(abstracts[n1].intersection(abstracts[n2]))\n",
        "    X_train[2*i+1,3] = G.degree(n1) + G.degree(n2)\n",
        "    X_train[2*i+1,4] = abs(G.degree(n1) - G.degree(n2))\n",
        "    X_train[2*i,5] = embeddings[edge[n1],:]\n",
        "    X_train[2*i,6] = embeddings[edge[n2],:]\n",
        "    y_train[2*i+1] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hUkgXmEchbP"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\n",
        "scores = validation_score(clf, embeddingd, y, cv = 4, n_jobs = 4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ALTEGRAD_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
