{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhGtlA7RYYpV"
      },
      "source": [
        "<center><h2>ALTEGRAD Project</h2>\n",
        "\n",
        "<hr>\n",
        "<span style=\"font-variant: small-caps;\">Xavier Jiménez</span><br>\n",
        "<hr>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1FMzZfeYMgQ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCeqWnCtUBiW",
        "outputId": "1caa3353-4986-4b40-e960-35483c5c1a34"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randint\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import log_loss, make_scorer\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import string\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# !pip install pip install karateclub\n",
        "from gensim.models.doc2vec import Doc2Vec\n",
        "from os import path\n",
        "import pickle\n",
        "from scipy import spatial\n",
        "import random\n",
        "try:\n",
        "    from google.colab import files\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already downloaded\n"
          ]
        }
      ],
      "source": [
        "if not path.isdir('data'):\n",
        "    !mkdir data\n",
        "    !wget -O altegrad.zip https://www.dropbox.com/sh/fhfjjtk0sr7pmse/AAD4ZEtHv9OI5HfVO22tdMX0a?dl=1\n",
        "    !unzip altegrad.zip\n",
        "else:\n",
        "    print('Data already downloaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NufV7kEnYJFo"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWDhmJpnVmCl",
        "outputId": "d75081de-3cb0-4049-86a9-2df0cf559397"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes: 138499\n",
            "Number of edges: 1091955\n"
          ]
        }
      ],
      "source": [
        "# Create a graph\n",
        "G = nx.read_edgelist('data/edgelist.txt', delimiter=',', create_using=nx.Graph(), nodetype=int)\n",
        "nodes = list(G.nodes())\n",
        "n = G.number_of_nodes()\n",
        "m = G.number_of_edges()\n",
        "print('Number of nodes:', n)\n",
        "print('Number of edges:', m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "18670it [00:00, 186697.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing authors\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "138499it [00:01, 103847.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'George W. Irwin', 'James H. Niblock', 'Jian-Xun Peng', 'Karen R. McMenemy'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Read the abstract of each paper\n",
        "try:\n",
        "    a_file = open(\"data/authors_preprocessed.pkl\", \"rb\")\n",
        "    authors = pickle.load(a_file)\n",
        "    a_file.close()\n",
        "    print('Authors already preprocessed')\n",
        "except:\n",
        "    print('Preprocessing authors')\n",
        "    authors = dict()\n",
        "    with open('data/authors.txt', 'r') as f:\n",
        "        for line in tqdm(f):\n",
        "            node, author = line.split('|--|')\n",
        "            # author = author.lower()\n",
        "            author = author.split(',')\n",
        "            author[-1] = author[-1].strip()\n",
        "            authors[int(node)] = set(author)\n",
        "        # for node in authors:\n",
        "        #     authors[node] = set(authors[node].split())\n",
        "        a_file = open(\"data/authors_preprocessed.pkl\", \"wb\")\n",
        "        pickle.dump(authors, a_file)\n",
        "        a_file.close()\n",
        "    print('Preprocessing Done')\n",
        "\n",
        "authors[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5j-YMepW46s",
        "outputId": "ec1883d8-fac2-423c-eb09-3e747c18f393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing abstracts\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "138499it [03:09, 730.64it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing Done\n"
          ]
        }
      ],
      "source": [
        "# Read the abstract of each paper\n",
        "try:\n",
        "    a_file = open(\"data/abstract_preprocessed.pkl\", \"rb\")\n",
        "    abstracts = pickle.load(a_file)\n",
        "    a_file.close()\n",
        "    print('Abstract already preprocessed')\n",
        "except:\n",
        "    print('Preprocessing abstracts')\n",
        "    stop_words = stopwords.words('english')\n",
        "    porter = PorterStemmer()\n",
        "    abstracts = dict()\n",
        "    with open('data/abstracts.txt', 'r') as f:\n",
        "        for line in tqdm(f):\n",
        "            node, abstract = line.split('|--|')\n",
        "            abstract = abstract.lower()\n",
        "            # abstract = \"\".join([char for char in abstract if char not in string.punctuation])\n",
        "            abstract = word_tokenize(abstract)\n",
        "            abstract = [word for word in abstract if word not in stop_words]\n",
        "            # abstract = [porter.stem(word) for word in abstract]\n",
        "            abstracts[int(node)] = abstract\n",
        "    a_file = open(\"data/abstract_preprocessed.pkl\", \"wb\")\n",
        "    pickle.dump(abstracts, a_file)\n",
        "    a_file.close()\n",
        "    print('Preprocessing Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['development',\n",
              " 'automated',\n",
              " 'system',\n",
              " 'quality',\n",
              " 'assessment',\n",
              " 'aerodrome',\n",
              " 'ground',\n",
              " 'lighting',\n",
              " '(',\n",
              " 'agl']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "abstracts[0][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5LbuWc0eF6U"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validation_score(model, X_train, y_train, cv, scoring = 'neg_log_loss', n_jobs = None, verbose = 0):\n",
        "    \"\"\"Computes scores using cross validation for a given model.\n",
        "\n",
        "    Args:\n",
        "        model: classifier\n",
        "        X_train (array like): training set.\n",
        "        y_train (array like): training lavels.\n",
        "        cv (int): number of splits.\n",
        "        scoring (str, optional): Metric. Defaults to 'neg_log_loss'.\n",
        "        n_jobs (int, optional): Number of cores. Defaults to None.\n",
        "        verbose (int, optional): Verbose level for cross_val_score\n",
        "\n",
        "    Returns:\n",
        "        list: list containing scores from cross validation\n",
        "    \"\"\"\n",
        "    print('Cross validation')\n",
        "    scores = cross_val_score(model, X_train, y_train, cv = cv, scoring = make_scorer(log_loss, greater_is_better = False, needs_proba = True), n_jobs = n_jobs, verbose = verbose)\n",
        "    print('Score: {:.2f} ± {:.2f}'.format(-scores.mean(), scores.std()/np.sqrt(cv)))\n",
        "    \n",
        "    return scores\n",
        "\n",
        "def create_submission(model, G, G_params, X_train, y_train):\n",
        "    # # Read test data. Each sample is a pair of nodes\n",
        "    print('Creating submission')\n",
        "    node_pairs = list()\n",
        "    with open('data/test.txt', 'r') as f:\n",
        "        for line in f:\n",
        "            t = line.split(',')\n",
        "            node_pairs.append((int(t[0]), int(t[1])))\n",
        "\n",
        "    X_test, _ = create_dataset(G, G_params, edges = node_pairs, training = False)\n",
        "    \n",
        "    # Use logistic regression to predict if two nodes are linked by an edge\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict_proba(X_test)\n",
        "    y_pred = y_pred[:,1]\n",
        "    df = pd.DataFrame(X_test)\n",
        "    print(df.head(15))\n",
        "\n",
        "    # Write predictions to a file\n",
        "    predictions = zip(range(len(y_pred)), y_pred)\n",
        "    os.remove(\"data/submission.csv\")\n",
        "    with open(\"data/submission.csv\",\"w\") as pred:\n",
        "        csv_out = csv.writer(pred)\n",
        "        csv_out.writerow(['id','predicted'])\n",
        "        for row in predictions:\n",
        "            csv_out.writerow(row)\n",
        "    print('Submision created')\n",
        "\n",
        "\n",
        "def graph_properties(G):\n",
        "    print('Computing graph properties')\n",
        "    avg_neighbor_degree = nx.average_neighbor_degree(G)\n",
        "    eig_centrality = nx.eigenvector_centrality_numpy(G)\n",
        "    pagerank = nx.pagerank_scipy(G)\n",
        "    greedy_color = nx.greedy_color(G)\n",
        "    triangles = nx.triangles(G)\n",
        "    core_number = nx.core_number(G)\n",
        "    onion_number = nx.onion_layers(G)\n",
        "    degree_centrality = nx.degree_centrality(G)\n",
        "    clustering = nx.clustering(G)\n",
        "\n",
        "    return [avg_neighbor_degree, eig_centrality, pagerank, greedy_color, triangles, core_number, onion_number, degree_centrality, clustering]\n",
        "            \n",
        "def create_dataset(G, G_params, edges = None, training = True): # \n",
        "\n",
        "    if training:\n",
        "        p = 2\n",
        "        edges = G.edges()\n",
        "    else:\n",
        "        p = 1\n",
        "    m = len(edges)\n",
        "    X_train = np.zeros((p*m, 6 + 2*len(G_params)))\n",
        "    y_train = np.zeros(p*m)\n",
        "    model = Doc2Vec.load(\"data/abstracts_embedding_doc2vec_vs128_w5_mc2_e100\")\n",
        "    for i,edge in tqdm(enumerate(edges)):\n",
        "        # an edge\n",
        "        X_train[p*i,0] = len(abstracts[edge[0]]) + len(abstracts[edge[1]])\n",
        "        X_train[p*i,1] = abs(len(abstracts[edge[0]]) - len(abstracts[edge[1]]))\n",
        "        X_train[p*i,2] = len(set(abstracts[edge[0]]).intersection(set(abstracts[edge[1]])))\n",
        "        X_train[p*i,3] = G.degree(edge[0]) + G.degree(edge[1])\n",
        "        X_train[p*i,4] = abs(G.degree(edge[0]) - G.degree(edge[1]))\n",
        "        # X_train[p*i,5] = len(authors[edge[0]].intersection(authors[edge[1]]))\n",
        "        cos_distance = spatial.distance.cosine(model[edge[0]], model[edge[1]])\n",
        "        # print(cos_distance)\n",
        "        X_train[p*i,5] = cos_distance\n",
        "        for j in range(6, len(G_params), 2):\n",
        "            param = G_params[j]\n",
        "            X_train[p*i,j] = param[edge[0]] + param[edge[1]]\n",
        "            X_train[p*i,j+1] = abs(param[edge[0]] - param[edge[1]])\n",
        "        \n",
        "        if training:\n",
        "            y_train[2*i] = 1\n",
        "\n",
        "            # a randomly generated pair of nodes\n",
        "            random.seed(2*1) \n",
        "            n1 = randint(0, n-1)\n",
        "            random.seed(2*i+1) \n",
        "            n2 = randint(0, n-1)\n",
        "            X_train[2*i+1,0] = len(abstracts[n1]) + len(abstracts[n2])\n",
        "            X_train[2*i+1,1] = abs(len(abstracts[n1]) - len(abstracts[n2]))\n",
        "            X_train[2*i+1,2] = len(set(abstracts[n1]).intersection(set(abstracts[n2])))\n",
        "            X_train[2*i+1,3] = G.degree(n1) + G.degree(n2)\n",
        "            X_train[2*i+1,4] = abs(G.degree(n1) - G.degree(n2))\n",
        "            # X_train[2*i+1,5] = len(authors[n1].intersection(abstracts[n2]))\n",
        "            cos_distance = spatial.distance.cosine(model[n1], model[n2])\n",
        "            # print(cos_distance)\n",
        "            X_train[2*i+1,5] = cos_distance\n",
        "            for j in range(6, len(G_params), 2):\n",
        "                param = G_params[j]\n",
        "                X_train[2*i+1,j] = param[n1] + param[n2]\n",
        "                X_train[2*i+1,j+1] = abs(param[n1] - param[n2])\n",
        "\n",
        "            y_train[2*i+1] = 0\n",
        "        \n",
        "    print('Size of the matrix:', X_train.shape)\n",
        "    \n",
        "    return X_train, y_train\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Computes node parameters for graph G and stores them in a list\n",
        "G_params = graph_properties(G)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, y_train = create_dataset(G, G_params = list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.DataFrame(data=X_train)\n",
        "df_train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test results with cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\n",
        "scores = validation_score(clf, X_train, y_train, cv = 4, n_jobs = 4, verbose = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = LogisticRegression()\n",
        "create_submission(clf, G, [], X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing embedding properties and other ideas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nodes_connected(G, u, v):\n",
        "    return u in G.neighbors(v)\n",
        "\n",
        "nodes_connected(G, 17, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected 0-0: 0.09, 4\n",
            "Connected 3-0: 0.59, 0\n",
            "Connected 5-0: 1.10, 0\n",
            "Connected 6-0: 0.65, 0\n",
            "Connected 7-0: 0.61, 0\n",
            "Connected 9-0: 0.69, 0\n",
            "Connected 10-0: 0.70, 1\n",
            "Connected 11-0: 0.54, 0\n",
            "Connected 12-0: 0.61, 0\n"
          ]
        }
      ],
      "source": [
        "d2v = Doc2Vec.load(\"data/abstracts_embedding_doc2vec_vs64_w5_mc2_e100\")\n",
        "\n",
        "neighbor = nx.single_source_shortest_path_length(G, 1, cutoff=3)\n",
        "# neighbor = {v: k for k, v in neighbor.items()}\n",
        "\n",
        "for i in list(neighbor.keys())[1:10]:\n",
        "    if nodes_connected(G, i, 1):\n",
        "        print('Connected {:d}-{:d}: {:.2f}, {:d}'.format(i, 0, spatial.distance.cosine(d2v[i], d2v[1]), len(authors[i].intersection(authors[0]))))\n",
        "    else:\n",
        "        print('Not Connected {:d}-{:d}: {:.2f}, {:d}'.format(i, 0, spatial.distance.cosine(d2v[i], d2v[1]), len(authors[i].intersection(authors[0]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not Connected 57843-0: 1.05\n",
            "Not Connected 37538-0: 0.93\n",
            "Not Connected 65147-0: 0.87\n",
            "Not Connected 106678-0: 0.93\n",
            "Not Connected 18223-0: 0.95\n",
            "Not Connected 89973-0: 0.91\n",
            "Not Connected 125515-0: 0.95\n",
            "Not Connected 73222-0: 1.09\n",
            "Not Connected 64208-0: 0.99\n",
            "Not Connected 4594-0: 0.94\n",
            "Not Connected 19913-0: 0.95\n",
            "Not Connected 57703-0: 0.89\n",
            "Not Connected 23905-0: 0.99\n",
            "Not Connected 122321-0: 0.92\n",
            "Not Connected 9201-0: 1.02\n",
            "Not Connected 101385-0: 0.98\n",
            "Not Connected 61178-0: 0.75\n",
            "Not Connected 88102-0: 0.87\n",
            "Not Connected 111660-0: 0.78\n",
            "Not Connected 90555-0: 1.03\n"
          ]
        }
      ],
      "source": [
        "for i in range(0,20):\n",
        "    # random.seed(i)\n",
        "    k1 = randint(0, n-1)\n",
        "    k2 = randint(0, n-1)\n",
        "    if nodes_connected(G, k1, k2):\n",
        "        print('Connected {:d}-{:d}: {:.2f}'.format(k1, 0, spatial.distance.cosine(d2v[k1], d2v[k2])))\n",
        "    else:\n",
        "        print('Not Connected {:d}-{:d}: {:.2f}'.format(k2, 0, spatial.distance.cosine(d2v[k1], d2v[k2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected 0-1: 0.09\n",
            "Connected 0-2: 0.86\n",
            "Connected 1-3: 0.59\n",
            "Connected 1-5: 1.10\n",
            "Connected 1-6: 0.65\n",
            "Connected 1-7: 0.61\n",
            "Connected 1-9: 0.69\n",
            "Connected 1-10: 0.70\n",
            "Connected 1-11: 0.54\n",
            "Connected 1-12: 0.61\n",
            "Connected 1-13: 0.58\n",
            "Connected 1-14: 0.59\n",
            "Connected 1-15: 0.67\n",
            "Connected 1-16: 0.66\n",
            "Connected 1-17: 0.84\n",
            "Connected 1-19: 1.00\n",
            "Connected 1-20: 0.66\n",
            "Connected 1-21: 0.74\n",
            "Connected 1-22: 0.60\n",
            "Connected 1-23: 0.61\n",
            "Connected 1-24: 0.72\n",
            "Connected 2-25: 0.82\n",
            "Connected 2-26: 0.91\n",
            "Connected 2-27: 0.99\n",
            "Connected 2-28: 1.00\n",
            "Connected 2-29: 0.99\n",
            "Connected 2-30: 1.05\n",
            "Connected 2-31: 1.04\n",
            "Connected 2-32: 0.87\n",
            "Connected 2-33: 0.92\n",
            "Connected 2-34: 0.93\n"
          ]
        }
      ],
      "source": [
        "for i, edge in enumerate(G.edges()):\n",
        "    print('Connected {:d}-{:d}: {:.2f}'.format(edge[0], edge[1], spatial.distance.cosine(d2v[edge[0]], d2v[edge[1]])))\n",
        "    if i==30:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20it [00:00, 4169.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "2\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for i,edge in tqdm(enumerate(G.edges())):\n",
        "    print(len(set(authors[edge[0]]).intersection(set(authors[edge[1]]))))\n",
        "    # cos_distance = spatial.distance.cosine(d2v[edge[0]], d2v[edge[1]])\n",
        "    # print(cos_distance)\n",
        "   \n",
        "    if i == 20:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ALTEGRAD_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
